{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":505351,"sourceType":"datasetVersion","datasetId":174469}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Machine Learning Skin Lesion Malignant or Benign using CNN**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Disclaimer**\n\n* Production Readiness and Model Quality: The primary objective of this project is to demonstrate a viable technical pipeline for model conversion, specifically from a JIT-compiled PyTorch model to a TFLite format for integration with Android using AI Edge Gallery and other platforms like Ollama/Gemma3N on an offline laptop. This model is not production-ready. It has not been subjected to rigorous performance, accuracy, or bias testing. The code serves as a proof-of-concept for the conversion process, and any further development would require extensive model improvement and validation before real-world deployment.\n\n* Not a Medical Tool; No Medical Advice: The output from this model, whether used in an Android app or integrated with an LLM like Gemma3N, is not a substitute for professional medical advice, diagnosis, or treatment. This model is an experimental tool for pattern recognition and should never be used as a replacement for consulting a qualified healthcare professional.\n\n* Framework-Specifics: This project outlines a successful conversion path, but the process may not be universally applicable to all PyTorch models. The success of the ONNX export and subsequent conversion depends heavily on the specific layers and operations used in the original PyTorch model. Certain custom or less common operations might not have direct ONNX or TensorFlow equivalents, leading to conversion failures.\n\n* Performance and Overhead: The use of tf.py_function to wrap the ONNX model is a powerful workaround, but it introduces overhead. The TFLite interpreter will need to perform a context switch to execute the native Python code via ONNX Runtime for every inference call. This can negate some of the performance benefits of using TFLite, especially on low-power devices. The final TFLite model is not a fully \"native\" TensorFlow Lite model.\n\n* TFLite Custom Operator Support: The resulting TFLite model relies on a custom operator to run the ONNX inference. The ability to run this custom TFLite model on a target device is dependent on the TFLite interpreter's support for custom ops. In many production scenarios, you would need to write a C++ implementation of the custom operator to make it portable and efficient.\n\n* Assumptions and Placeholders: The code contains assumptions, such as the file path /kaggle/input/... and a placeholder for the output shape within tf.py_function. These must be carefully updated and validated for any specific use case. The project assumes a JIT-compiled PyTorch model, and the process for non-JIT models might be slightly different.\n\n* Lack of Quantization: The pipeline does not include any model quantization, which is a critical step for deploying models on resource-constrained devices to reduce size and improve latency.\n\n* This is utilized for working on an output for skin cancer with the existing dataset in Kaggle. If there is more data then definitely all of the classifications would yield a better output.  Data and Generalization: The effectiveness of the final TFLite model is entirely dependent on the quality, diversity, and quantity of the data used to train the original PyTorch model. If the training data for the \"skin cancer model\" did not include a wide range of skin types, cancer types, or lighting conditions, the model may perform poorly when deployed in the real world. The conversion process itself does not address or mitigate any biases or deficiencies in the original training data.","metadata":{}},{"cell_type":"markdown","source":" ","metadata":{}},{"cell_type":"markdown","source":"# **Model Training Data**\n\n\nhttps://www.kaggle.com/datasets/fanconic/skin-cancer-malignant-vs-benign","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Model Training Code**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\n# âš™ï¸ Device setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T17:03:16.796587Z","iopub.execute_input":"2025-08-01T17:03:16.796970Z","iopub.status.idle":"2025-08-01T17:03:30.613954Z","shell.execute_reply.started":"2025-08-01T17:03:16.796934Z","shell.execute_reply":"2025-08-01T17:03:30.612443Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"class CustomCNN(nn.Module):\n    def __init__(self):\n        super(CustomCNN, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n        )\n\n        # Adaptive to any input size\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(128 * 14 * 14, 256),  # Adjust depending on input size\n            nn.ReLU(),\n            nn.Linear(256, 1)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T17:03:30.615551Z","iopub.execute_input":"2025-08-01T17:03:30.616122Z","iopub.status.idle":"2025-08-01T17:03:30.625935Z","shell.execute_reply.started":"2025-08-01T17:03:30.616092Z","shell.execute_reply":"2025-08-01T17:03:30.624235Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"%pwd\n%ls -al /kaggle/input/skin-cancer-malignant-vs-benign/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T17:03:30.628985Z","iopub.execute_input":"2025-08-01T17:03:30.629709Z","iopub.status.idle":"2025-08-01T17:03:30.821657Z","shell.execute_reply.started":"2025-08-01T17:03:30.629673Z","shell.execute_reply":"2025-08-01T17:03:30.818993Z"}},"outputs":[{"name":"stdout","text":"total 4\ndrwxr-xr-x 5 nobody nogroup    0 Jul 11 18:44 \u001b[0m\u001b[01;34m.\u001b[0m/\ndrwxr-xr-x 3 root   root    4096 Aug  1 17:02 \u001b[01;34m..\u001b[0m/\ndrwxr-xr-x 4 nobody nogroup    0 Jul 11 18:44 \u001b[01;34mdata\u001b[0m/\ndrwxr-xr-x 4 nobody nogroup    0 Jul 11 18:44 \u001b[01;34mtest\u001b[0m/\ndrwxr-xr-x 4 nobody nogroup    0 Jul 11 18:44 \u001b[01;34mtrain\u001b[0m/\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%pwd\n%ls -al /kaggle/input/skin-cancer-malignant-vs-benign/data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T17:03:30.823291Z","iopub.execute_input":"2025-08-01T17:03:30.823719Z","iopub.status.idle":"2025-08-01T17:03:30.982936Z","shell.execute_reply.started":"2025-08-01T17:03:30.823681Z","shell.execute_reply":"2025-08-01T17:03:30.980956Z"}},"outputs":[{"name":"stdout","text":"total 0\ndrwxr-xr-x 4 nobody nogroup 0 Jul 11 18:44 \u001b[0m\u001b[01;34m.\u001b[0m/\ndrwxr-xr-x 5 nobody nogroup 0 Jul 11 18:44 \u001b[01;34m..\u001b[0m/\ndrwxr-xr-x 4 nobody nogroup 0 Jul 11 18:44 \u001b[01;34mtest\u001b[0m/\ndrwxr-xr-x 4 nobody nogroup 0 Jul 11 18:44 \u001b[01;34mtrain\u001b[0m/\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ðŸ§¼ Image preprocessing\nimg_size = 112\ntransform = transforms.Compose([\n    transforms.Resize((img_size, img_size)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# ðŸ“‚ Dataset setup\ntrain_path = '/kaggle/input/skin-cancer-malignant-vs-benign/train'\nval_path = '/kaggle/input/skin-cancer-malignant-vs-benign/test'\n\ntrain_data = datasets.ImageFolder(train_path, transform=transform)\nval_data = datasets.ImageFolder(val_path, transform=transform)\n\ntrain_loader = DataLoader(train_data, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=64, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T17:03:30.985114Z","iopub.execute_input":"2025-08-01T17:03:30.985605Z","iopub.status.idle":"2025-08-01T17:03:37.950887Z","shell.execute_reply.started":"2025-08-01T17:03:30.985568Z","shell.execute_reply":"2025-08-01T17:03:37.949536Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ðŸ“¦ Model, Loss, Optimizer\nmodel = CustomCNN().to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T17:03:37.952519Z","iopub.execute_input":"2025-08-01T17:03:37.952860Z","iopub.status.idle":"2025-08-01T17:03:38.062171Z","shell.execute_reply.started":"2025-08-01T17:03:37.952835Z","shell.execute_reply":"2025-08-01T17:03:38.059746Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, epochs=10):\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n\n        for inputs, labels in train_loader:\n            inputs = inputs.to(device)\n            labels = labels.float().unsqueeze(1).to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        avg_train_loss = total_loss / len(train_loader)\n\n        # ðŸ“Š Validation\n        model.eval()\n        all_preds = []\n        all_labels = []\n\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs = inputs.to(device)\n                labels = labels.float().unsqueeze(1).to(device)\n                outputs = model(inputs)\n                preds = (torch.sigmoid(outputs) > 0.5).float()\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n\n        acc = accuracy_score(all_labels, all_preds)\n\n        print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_train_loss:.4f} | Val Accuracy: {acc:.4f}\")\n\ntrain_model(model, train_loader, val_loader, epochs=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T17:03:38.063892Z","iopub.execute_input":"2025-08-01T17:03:38.064475Z","iopub.status.idle":"2025-08-01T17:18:56.011270Z","shell.execute_reply.started":"2025-08-01T17:03:38.064437Z","shell.execute_reply":"2025-08-01T17:18:56.010130Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10 | Loss: 0.4427 | Val Accuracy: 0.8242\nEpoch 2/10 | Loss: 0.3694 | Val Accuracy: 0.8500\nEpoch 3/10 | Loss: 0.3323 | Val Accuracy: 0.8591\nEpoch 4/10 | Loss: 0.2939 | Val Accuracy: 0.8515\nEpoch 5/10 | Loss: 0.2744 | Val Accuracy: 0.8197\nEpoch 6/10 | Loss: 0.2370 | Val Accuracy: 0.8530\nEpoch 7/10 | Loss: 0.2227 | Val Accuracy: 0.8636\nEpoch 8/10 | Loss: 0.1898 | Val Accuracy: 0.8500\nEpoch 9/10 | Loss: 0.1820 | Val Accuracy: 0.8576\nEpoch 10/10 | Loss: 0.1512 | Val Accuracy: 0.8682\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Model Results**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\ndef evaluate_model(model, val_loader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs = inputs.to(device)\n            labels = labels.float().unsqueeze(1).to(device)\n\n            outputs = model(inputs)\n            preds = (torch.sigmoid(outputs) > 0.5).float()\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    # Convert to numpy arrays\n    y_true = np.array(all_labels)\n    y_pred = np.array(all_preds)\n\n    acc = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n\n    print(\"\\nðŸ“Š Final Evaluation Metrics:\")\n    print(f\"âœ… Accuracy : {acc:.4f}\")\n    print(f\"ðŸŽ¯ Precision: {precision:.4f}\")\n    print(f\"ðŸ” Recall   : {recall:.4f}\")\n    print(f\"ðŸ“ˆ F1 Score : {f1:.4f}\")\n\n    return acc, precision, recall, f1\n\n# Call this after training\nevaluate_model(model, val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T17:24:29.663509Z","iopub.execute_input":"2025-08-01T17:24:29.664045Z","iopub.status.idle":"2025-08-01T17:24:40.786259Z","shell.execute_reply.started":"2025-08-01T17:24:29.664004Z","shell.execute_reply":"2025-08-01T17:24:40.784976Z"}},"outputs":[{"name":"stdout","text":"\nðŸ“Š Final Evaluation Metrics:\nâœ… Accuracy : 0.8682\nðŸŽ¯ Precision: 0.8257\nðŸ” Recall   : 0.9000\nðŸ“ˆ F1 Score : 0.8612\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(0.8681818181818182, 0.8256880733944955, 0.9, 0.861244019138756)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\n\ndef predict_image(model, image_path, transform, threshold=0.5):\n    \"\"\"\n    Predicts the class of a single image using the trained model.\n    \"\"\"\n    model.eval()\n\n    # Load and preprocess image\n    image = Image.open(image_path).convert('RGB')\n    input_tensor = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n\n    with torch.no_grad():\n        output = model(input_tensor)\n        prob = torch.sigmoid(output).item()\n        prediction = 1 if prob > threshold else 0\n\n    print(f\"\\nðŸ–¼ï¸ Prediction Result for '{image_path}':\")\n    print(f\"ðŸ”¢ Raw Score (Sigmoid): {prob:.4f}\")\n    print(f\"ðŸ§  Predicted Class     : {'Malignant' if prediction == 1 else 'Benign'}\")\n\n    return prediction, prob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T17:28:59.697278Z","iopub.execute_input":"2025-08-01T17:28:59.699341Z","iopub.status.idle":"2025-08-01T17:28:59.709823Z","shell.execute_reply.started":"2025-08-01T17:28:59.699278Z","shell.execute_reply":"2025-08-01T17:28:59.707865Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# ðŸ” Replace this with your test image path\ntest_image_path = \"/kaggle/input/skin-cancer-malignant-vs-benign/test/benign/1.jpg\"\n \n\n# ðŸ”® Predict\npredict_image(model, test_image_path, transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T17:29:01.969694Z","iopub.execute_input":"2025-08-01T17:29:01.970074Z","iopub.status.idle":"2025-08-01T17:29:02.001854Z","shell.execute_reply.started":"2025-08-01T17:29:01.970049Z","shell.execute_reply":"2025-08-01T17:29:01.999429Z"}},"outputs":[{"name":"stdout","text":"\nðŸ–¼ï¸ Prediction Result for '/kaggle/input/skin-cancer-malignant-vs-benign/test/benign/1.jpg':\nðŸ”¢ Raw Score (Sigmoid): 0.3693\nðŸ§  Predicted Class     : Benign\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(0, 0.36933571100234985)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# ðŸ” Replace this with your test image path\ntest_image_path = \"/kaggle/input/skin-cancer-malignant-vs-benign/test/malignant/1.jpg\"\n \n\n# ðŸ”® Predict\npredict_image(model, test_image_path, transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T17:29:05.314256Z","iopub.execute_input":"2025-08-01T17:29:05.314659Z","iopub.status.idle":"2025-08-01T17:29:05.341151Z","shell.execute_reply.started":"2025-08-01T17:29:05.314634Z","shell.execute_reply":"2025-08-01T17:29:05.339018Z"}},"outputs":[{"name":"stdout","text":"\nðŸ–¼ï¸ Prediction Result for '/kaggle/input/skin-cancer-malignant-vs-benign/test/malignant/1.jpg':\nðŸ”¢ Raw Score (Sigmoid): 0.9931\nðŸ§  Predicted Class     : Malignant\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(1, 0.9931436777114868)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Save the model**","metadata":{}},{"cell_type":"code","source":"# ðŸ” Save PyTorch model\ntorch.save(model.state_dict(), \"custom_cnn.pth\")\n\n# ðŸ§ª Convert to TorchScript for mobile\nexample_input = torch.randn(1, 3, img_size, img_size).to(device)\ntraced_model = torch.jit.trace(model, example_input)\ntraced_model.save(\"custom_cnn_mobile.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T17:29:08.137030Z","iopub.execute_input":"2025-08-01T17:29:08.137723Z","iopub.status.idle":"2025-08-01T17:29:08.574251Z","shell.execute_reply.started":"2025-08-01T17:29:08.137682Z","shell.execute_reply":"2025-08-01T17:29:08.572844Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"%pwd\n%ls -al","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T17:29:10.920337Z","iopub.execute_input":"2025-08-01T17:29:10.920729Z","iopub.status.idle":"2025-08-01T17:29:11.079559Z","shell.execute_reply.started":"2025-08-01T17:29:10.920704Z","shell.execute_reply":"2025-08-01T17:29:11.077817Z"}},"outputs":[{"name":"stdout","text":"total 50980\ndrwxr-xr-x 3 root root     4096 Aug  1 17:29 \u001b[0m\u001b[01;34m.\u001b[0m/\ndrwxr-xr-x 5 root root     4096 Aug  1 17:02 \u001b[01;34m..\u001b[0m/\n-rw-r--r-- 1 root root 26110150 Aug  1 17:29 custom_cnn_mobile.pt\n-rw-r--r-- 1 root root 26076962 Aug  1 17:29 custom_cnn.pth\ndrwxr-xr-x 2 root root     4096 Aug  1 17:02 \u001b[01;34m.virtual_documents\u001b[0m/\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Outputting model to PyTorch PT format and also TFLite for Mobile and Laptop integration with LLM**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This is on my laptop when I ran the script below manually via command prompt on a Windows 11 (this was painful to copy and modify but I hope you appreciate this):\n>\n> pip list\n>\n> Package                  Version\n------------------------ -----------\n> \n> absl-py                  2.3.1\n> \n> aiohappyeyeballs         2.6.1\n> \n> aiohttp                  3.12.15\n> \n> aiosignal                1.4.0\n> \n> annotated-types          0.7.0\n> \n> anyio                    4.9.0\n> \n> astunparse               1.6.3\n> \n> attrs                    25.3.0\n> \n> certifi                  2025.7.14\n> \n> charset-normalizer       3.4.2\n> \n> click                    8.2.1\n> \n> colorama                 0.4.6\n> \n> coloredlogs              15.0.1\n> \n> dataclasses-json         0.6.7\n> \n> fastapi                  0.116.1\n> \n> filelock                 3.18.0\n> \n> flatbuffers              25.2.10\n> \n> frozenlist               1.7.0\n> \n> fsspec                   2025.7.0\n> \n> gast                     0.6.0\n> \n> google-pasta             0.2.0\n> \n> greenlet                 3.2.3\n> \n> grpcio                   1.74.0\n> \n> h11                      0.16.0\n> \n> h5py                     3.14.0\n> \n> httpcore                 1.0.9\n> \n> httpx                    0.28.1\n> \n> httpx-sse                0.4.1\n> \n> humanfriendly            10.0\n> \n> idna                     3.10\n> \n> intel-cmplr-lic-rt       2025.2.0\n> \n> intel-opencl-rt          2025.2.0\n> \n> Jinja2                   3.1.6\n> \n> joblib                   1.5.1\n> \n> jsonpatch                1.33\n> \n> jsonpointer              3.0.0\n> \n> keras                    3.11.1\n> \n> langchain                0.3.27\n> \n> langchain-community      0.3.27\n> \n> langchain-core           0.3.72\n> \n> langchain-text-splitters 0.3.9\n> \n> langsmith                0.4.8\n> \n> libclang                 18.1.1\n> \n> Markdown                 3.8.2\n> \n> markdown-it-py           3.0.0\n> \n> MarkupSafe               3.0.2\n> \n> marshmallow              3.26.1\n> \n> mdurl                    0.1.2\n> \n> ml_dtypes                0.5.3\n> \n> mpmath                   1.3.0\n> \n> multidict                6.6.3\n> \n> mypy_extensions          1.1.0\n> \n> namex                    0.1.0\n> \n> networkx                 3.5\n> \n> numpy                    2.3.2\n> \n> ollama                   0.5.1\n> \n> onnx                     1.18.0\n> \n> onnx-tf                  1.6.0\n> \n> onnxruntime              1.22.1\n> \n> opt_einsum               3.4.0\n> \n> optree                   0.17.0\n> \n> orjson                   3.11.1\n> \n> packaging                25.0\n> \n> pandas                   2.3.1\n> \n> pillow                   11.3.0\n> \n> pip                      25.1.1\n> \n> propcache                0.3.2\n> \n> protobuf                 6.31.1\n> \n> pydantic                 2.11.7\n> \n> pydantic_core            2.33.2\n>\n> pydantic-settings        2.10.1\n> \n> Pygments                 2.19.2\n> \n> pyreadline3              3.5.4\n> \n> python-dateutil          2.9.0.post0\n> \n> python-dotenv            1.1.1\n> \n> pytz                     2025.2\n> \n> PyYAML                   6.0.2\n> \n> requests                 2.32.4\n> \n> requests-toolbelt        1.0.0\n> \n> rich                     14.1.0\n> \n> scikit-learn             1.7.1\n> \n> scipy                    1.16.1\n> \n> setuptools               80.9.0\n> \n> six                      1.17.0\n> \n> sniffio                  1.3.1\n>\n> SQLAlchemy               2.0.42\n> \n> starlette                0.47.2\n> \n> sympy                    1.14.0\n> \n> tbb                      2022.2.0\n> \n> tcmlib                   1.4.0\n> \n> tenacity                 9.1.2\n> \n> tensorboard              2.20.0\n> \n> tensorboard-data-server  0.7.2\n> \n> tensorflow               2.20.0rc0\n> \n> termcolor                3.1.0\n> \n> threadpoolctl            3.6.0\n> \n> torch                    2.7.1\n> \n> torchvision              0.22.1\n> \n> typing_extensions        4.14.1\n> \n> typing-inspect           0.9.0\n> \n> typing-inspection        0.4.1\n> \n> tzdata                   2025.2\n> \n> urllib3                  2.5.0\n> \n> uvicorn                  0.35.0\n> \n> Werkzeug                 3.1.3\n> \n> wheel                    0.45.1\n> \n> wrapt                    1.17.2\n> \n> yarl                     1.20.1\n> \n> zstandard                0.23.0 \n","metadata":{}},{"cell_type":"code","source":"pip install tf2onnx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T19:19:30.704690Z","iopub.execute_input":"2025-08-01T19:19:30.705065Z","iopub.status.idle":"2025-08-01T19:22:52.326482Z","shell.execute_reply.started":"2025-08-01T19:19:30.705015Z","shell.execute_reply":"2025-08-01T19:22:52.324784Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7970ed8c7f50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/tf2onnx/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7970ebb10f10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/tf2onnx/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7970ebafeb10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/tf2onnx/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7970ebb74950>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/tf2onnx/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7970ebb95f50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/tf2onnx/\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement tf2onnx (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for tf2onnx\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"pip install onnxruntime-rocm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install onnx onnx-tf ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T19:12:03.941295Z","iopub.execute_input":"2025-08-01T19:12:03.941533Z","iopub.status.idle":"2025-08-01T19:15:25.415779Z","shell.execute_reply.started":"2025-08-01T19:12:03.941505Z","shell.execute_reply":"2025-08-01T19:15:25.414268Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (1.18.0)\n\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e6cd7f422d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/onnx-tf/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e6cd7f3e450>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/onnx-tf/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e6cd802d3d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/onnx-tf/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e6cd7ebf690>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/onnx-tf/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e6cd7ebc990>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/onnx-tf/\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement onnx-tf (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for onnx-tf\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install onnxruntime keras","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T19:15:25.417250Z","iopub.execute_input":"2025-08-01T19:15:25.417654Z","iopub.status.idle":"2025-08-01T19:18:47.517102Z","shell.execute_reply.started":"2025-08-01T19:15:25.417592Z","shell.execute_reply":"2025-08-01T19:18:47.515684Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dd165f94c90>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/onnxruntime/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dd165f7e190>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/onnxruntime/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dd165f66250>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/onnxruntime/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dd165fb2510>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/onnxruntime/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dd165f9de50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/onnxruntime/\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement onnxruntime (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for onnxruntime\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# You may have to restart the cluster, but keep the .pt file available to run below\n# You could also put all of these pip install to the top of the notebook and then run below later.\n# I put these here so that you know what packages are needed to convert the Pytorch .pt model to onnx and tflite to integrate with AI Edge Gallery or Python/Ollama","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Output this in the format for edge devices (Laptop/Ollama/Gemma3 for phase 1 and Android in phase 2)\n# I made this run locally on my laptop but it also works on Kaggle. To make this work on your laptop, just download the .pt file into a folder and change the paths.\nimport torch\nimport onnx\nimport onnxruntime as ort\nimport tensorflow as tf\nfrom tensorflow import keras\nimport os\nimport numpy as np\n\n# --- Part 1: Export the PyTorch JIT model to ONNX ---\n\n# Load the JIT-compiled model. Note: The path below is a placeholder and should be updated.\ndevice = torch.device(\"cpu\")\ntry:\n    cnn_model = torch.jit.load(\"/kaggle/working/custom_cnn_mobile.pt\", map_location=device)\n    cnn_model.eval()\nexcept FileNotFoundError:\n    print(\"Error: PyTorch model not found. Please verify the path.\")\n    # Exit or handle the error gracefully\n    cnn_model = None\n\nif cnn_model:\n    # Create the dummy input tensor with the correct size.\n    # The shape should match the expected input of your PyTorch model.\n    dummy_input = torch.randn(1, 3, 112, 112, requires_grad=True)\n\n    # Export the JIT model to ONNX format.\n    onnx_model_path = \"custom_cnn_mobile_model.onnx\"\n    torch.onnx.export(cnn_model,\n                      dummy_input,\n                      onnx_model_path,\n                      opset_version=11,\n                      input_names=['input'],\n                      output_names=['output'],\n                      dynamic_axes={'input' : {0 : 'batch_size'}})\n\n    print(\"Step 1: JIT model has been successfully exported to ONNX.\")\n\n    # --- Part 2: Convert ONNX to a Keras model and then to TFLite ---\n\n    # Use ONNX Runtime to get the model's output\n    sess = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n    output_name = sess.get_outputs()[0].name\n    input_name = sess.get_inputs()[0].name\n    \n    # Create a helper function to run ONNX inference, which will be called by tf.py_function\n    def _run_onnx_inference(input_data):\n        \"\"\"Helper function to run the ONNX session with a numpy array.\"\"\"\n        # Ensure the input is a contiguous numpy array, which is often required\n        # by ONNX Runtime and can resolve data format issues from tf.py_function.\n        outputs = sess.run([output_name], {input_name: np.ascontiguousarray(input_data)})\n        return outputs[0]\n\n    # Create a simple Keras model that wraps the ONNX Runtime session.\n    class ONNXModelWrapper(keras.Model):\n        def __init__(self, onnx_session, output_name, input_name):\n            super(ONNXModelWrapper, self).__init__()\n            self.output_name = output_name\n            self.input_name = input_name\n\n        @tf.function(input_signature=[\n            tf.TensorSpec(shape=(None, 3, 112, 112), dtype=tf.float32)\n        ])\n        def call(self, inputs):\n            # Use tf.py_function to wrap the ONNX inference call.\n            # We must specify the output shape and dtype to ensure the graph\n            # is correctly traced.\n            return tf.py_function(\n                _run_onnx_inference,\n                inp=[inputs],\n                Tout=tf.float32,\n                # Explicitly set the output shape to allow for proper graph building.\n                # The shape can be inferred from the model's output. For this example,\n                # we'll assume a known output shape after the first dimension.\n                # You might need to adjust this based on your model's output.\n                # Example: tf.TensorShape([None, 1000]) for a 1000-class classifier\n                # We will return a placeholder for the output shape here\n                # since it's difficult to determine without knowing the model.\n                # In practice, you would calculate this from the ONNX model's output.\n                name=\"onnx_inference_op\"\n            )\n\n    # Instantiate the wrapper model.\n    onnx_model_wrapper = ONNXModelWrapper(sess, output_name, input_name)\n    onnx_model_wrapper.build(input_shape=(None, 3, 112, 112))\n    \n    # Call the model once with dummy data to build the graph before exporting.\n    dummy_input_tf = tf.convert_to_tensor(dummy_input.detach().numpy())\n    _ = onnx_model_wrapper(dummy_input_tf)\n\n    # Save the wrapper as a TensorFlow SavedModel.\n    tf_saved_model_path = \"tf_saved_model_final\"\n    onnx_model_wrapper.export(tf_saved_model_path)\n\n    print(\"Step 2: ONNX model successfully wrapped in a Keras model and saved.\")\n\n    # --- Part 3: Convert the SavedModel to TFLite, allowing custom ops ---\n\n    tflite_model_path = \"custom_cnn_mobile_model.tflite\"\n\n    # Convert the SavedModel to a TFLite model using the native converter.\n    # The key change here is to set `allow_custom_ops=True`.\n    # This instructs the converter to keep `tf.py_function` as a custom operation\n    # instead of trying to convert it to a native TFLite op, which caused the error.\n    converter = tf.lite.TFLiteConverter.from_saved_model(tf_saved_model_path)\n    converter.allow_custom_ops = True\n    tflite_model = converter.convert()\n\n    # Save the TFLite model to a file.\n    with open(tflite_model_path, \"wb\") as f:\n        f.write(tflite_model)\n\n    print(\"Step 3: SavedModel successfully converted to TFLite format.\")\n    print(f\"Final TFLite file saved at: {tflite_model_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Understanding the output and interpreting the data**# ","metadata":{}},{"cell_type":"markdown","source":"Based on the model's output, here is a summary of the performance:\n\nFinal Evaluation Metrics:\n\nAccuracy: 0.8682\n\nPrecision: 0.8257\n\nRecall: 0.9000\n\nF1 Score: 0.8612\n\nPerformance Over Epochs:\n\nThe model was trained for 10 epochs.\n\nLoss: The training loss consistently decreased from 0.4427 in the first epoch to 0.1512 in the final epoch.\n\nValidation Accuracy: The validation accuracy fluctuated during training, with the highest value of 0.8682 achieved in the final epoch (10 \nth\n  epoch). The lowest validation accuracy was 0.8197, observed in the 5 \nth\n  epoch.\n  \n\nFinal Evaluation Metrics\nThese numbers represent how well the model performed on a final, unseen test dataset after training was complete.\n\nAccuracy (0.8682): This means the model made the correct prediction about 86.82% of the time. It's a good general measure, but it can be misleading for imbalanced datasets.\n\nPrecision (0.8257): When the model predicted something was positive, it was correct 82.57% of the time. This metric focuses on the quality of the positive predictions and tells you how trustworthy the model's \"yes\" answers are.\n\nRecall (0.9000): The model successfully identified 90.00% of all actual positive cases. This metric focuses on the model's ability to find all the positive cases and tells you how good the model is at not missing things.\n\nF1 Score (0.8612): This is the harmonic mean of Precision and Recall. It provides a single score that balances both metrics. A high F1 Score indicates that the model is both good at not making false positive predictions (high precision) and good at not missing positive cases (high recall).\n\nPerformance Over Epochs\nAn \"epoch\" is one complete pass through the entire training dataset. These numbers show the model's learning progress over time.\n\nLoss: This measures the model's error. A decreasing loss (from 0.4427 to 0.1512) is a good sign, as it means the model is getting better at making predictions on the training data.\n\nValidation Accuracy: This measures the model's performance on a separate \"validation\" dataset that it hasn't seen before. The goal is for this number to increase. The validation accuracy generally improved over the 10 epochs, reaching its highest point (0.8682) in the final epoch. The fluctuations are normal and indicate the model is adjusting its parameters.\n\nOverall Conclusion\nThe model is performing very well. The consistently decreasing loss shows that it learned effectively from the training data. The final metrics are strong, with a high accuracy and a great balance between precision and recall, as indicated by the high F1 Score. The model is particularly good at identifying positive cases (high recall) and is reasonably accurate when it makes a positive prediction (good precision).","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Summary of What I Learned**\n\nThis project provided a deep and practical understanding of the challenges and solutions involved in modern machine learning model deployment. The key learnings can be summarized as follows:\n\n* Model Interoperability is Not a Given: While standards like ONNX exist to bridge frameworks, the conversion process is not always a simple one-to-one mapping. It often requires creative workarounds and a solid understanding of how each framework represents and executes models.\n\n* The Power of tf.py_function: I learned that tf.py_function is an essential tool in the TensorFlow ecosystem for integrating custom, non-TensorFlow logic into a TensorFlow graph. It's a key strategy for handling complex, framework-specific operations that would otherwise be impossible to convert directly.\n\n* The Role of Intermediate Formats: ONNX is not just a format; it's a critical bridge in the model lifecycle. I learned to appreciate its role in decoupling the training framework (PyTorch) from the inference engine (TensorFlow Lite via ONNX Runtime), providing greater flexibility and choice.\n\n* The Importance of a Well-Defined Pipeline: The project demonstrated the need for a systematic, multi-step pipeline for model conversion. Each stepâ€”from exporting to an intermediate format to wrapping with custom logic and finally converting for a specific targetâ€”is a distinct phase with its own set of requirements and potential pitfalls.\n\n* Deployment-Oriented Mindset: I learned that model development doesn't end with training. The entire process, from model design to the final conversion pipeline, should be considered with the deployment target in mind. The choice to use allow_custom_ops and the careful handling of shapes and data types are examples of decisions driven by the end goal of TFLite deployment.\n","metadata":{}}]}